---
title: "Design Generation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Design Generation}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  \usepackage{xcolor}
  \usepackage{bbding}
bibliography: "`r here::here('vignettes', 'library.bib')`"
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.retina = 3,
  comment = "#>"
)
```

Once you have a set of profiles and (optionally) priors, you can generate a choice-based conjoint (CBC) survey design using the `cbc_design()` function. This article covers all the design methods available, their features, and how to customize designs for specific research needs.

```{r}
library(cbcTools)

# Example profiles for demonstrations
profiles <- cbc_profiles(
  price     = c(1, 1.5, 2, 2.5, 3),
  type      = c('Fuji', 'Gala', 'Honeycrisp'),
  freshness = c('Poor', 'Average', 'Excellent')
)

# Example priors for Bayesian methods
priors <- cbc_priors(
  profiles = profiles,
  price = -0.1,
  type = c(0.1, 0.2),
  freshness = c(0.1, 0.2)
)
```

# Design Methods Overview

The `cbc_design()` function supports several design generation methods, each with different strengths and use cases:

## Method Comparison Table

| Method       | Speed | Efficiency | No Choice | Labeled | Restrictions | Blocking | Interactions | Priors Required |
|--------------|-------|------------|-----------|---------|--------------|----------|--------------|-----------------|
| `"random"`   | Fast  | Low        | ✓         | ✓       | ✓            | ✗        | ✓            | ✗               |
| `"shortcut"` | Fast  | Medium     | ✓         | ✓       | ✓            | ✗        | ✗            | ✗               |
| `"minoverlap"` | Fast | Medium   | ✓         | ✓       | ✓            | ✗        | ✗            | ✗               |
| `"balanced"` | Fast  | Medium     | ✓         | ✓       | ✓            | ✗        | ✗            | ✗               |
| `"stochastic"` | Slow | High     | ✓         | ✓       | ✓            | ✓        | ✓            | Recommended     |
| `"modfed"`   | Slow  | High       | ✓         | ✓       | ✓            | ✓        | ✓            | Recommended     |
| `"cea"`      | Slow  | High       | ✓         | ✓       | ✗*           | ✓        | ✓            | Recommended     |

*CEA requires full factorial profiles (no restrictions)

## Design Quality Guarantees

All design methods ensure:

1. **No duplicate profiles** within any choice set
2. **No duplicate choice sets** within any respondent
3. **Dominance removal** (if enabled) eliminates choice sets with dominant alternatives

# Random Design

The `"random"` method is the default and creates designs by randomly sampling profiles for each respondent independently. This ensures maximum diversity but may be less statistically efficient.

```{r}
design_random <- cbc_design(
  profiles = profiles,
  n_alts = 2,    # Alternatives per question
  n_q = 6,       # Questions per respondent
  n_resp = 100,  # Number of respondents
  method = "random"
)

# Quick inspection
cbc_inspect(design_random, sections = "structure")
```

**When to use:**

- Large sample sizes where efficiency matters less
- Want maximum diversity across respondents
- No strong prior assumptions about parameters
- Quick prototyping or testing

# Frequency-Based Methods

These methods use greedy algorithms to balance attribute level frequencies and minimize overlap. While they prioritize different metrics, they often can result in similar solutions.

## Shortcut Method

The `"shortcut"` method balances attribute level frequencies while avoiding duplicate profiles within questions.

```{r}
design_shortcut <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  method = "shortcut"
)
```

## Minimum Overlap Method

The `"minoverlap"` method prioritizes minimizing attribute overlap within choice questions.

```{r}
design_minoverlap <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  method = "minoverlap"
)
```

## Balanced Method

The `"balanced"` method optimizes both frequency balance and pairwise attribute interactions.

```{r}
design_balanced <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  method = "balanced"
)

# Compare balance across methods
cat("Shortcut balance:\n")
cbc_inspect(design_shortcut, sections = "balance")

cat("\nMinimum overlap balance:\n")
cbc_inspect(design_minoverlap, sections = "balance")

cat("\nBalanced method balance:\n")
cbc_inspect(design_balanced, sections = "balance")
```

# D-Optimal Methods

These methods minimize D-error to create statistically efficient designs. They require more computation but produce higher-quality designs, especially with good priors.

## Stochastic Optimization

The `"stochastic"` method uses random profile swapping and accepts the first improvement found.

```{r}
design_stochastic <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  priors = priors,
  method = "stochastic",
  n_start = 3  # Number of random starting points
)
```

## Modified Fedorov Algorithm

The `"modfed"` method exhaustively tests all possible profile swaps for each position.

```{r}
design_modfed <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  priors = priors,
  method = "modfed",
  n_start = 3
)
```

## Coordinate Exchange Algorithm

The `"cea"` method optimizes attribute-by-attribute, testing all possible levels for each attribute.

```{r}
design_cea <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  priors = priors,
  method = "cea",
  n_start = 3
)

# Compare D-errors
cat("Method comparison (D-error, lower is better):\n")
cat("Stochastic:", attr(design_stochastic, "design_params")$d_error_prior, "\n")
cat("ModFed:    ", attr(design_modfed, "design_params")$d_error_prior, "\n")
cat("CEA:       ", attr(design_cea, "design_params")$d_error_prior, "\n")
```

# Design Features

## No-Choice Option

Add a "no-choice" alternative to allow respondents to opt out by including the argument `no_choice = TRUE`. If you are using priors, then you must also provide a `no_choice` value in your priors:

```{r}
# For D-optimal methods, must include no_choice in priors
priors_nochoice <- cbc_priors(
  profiles = profiles,
  price = -0.1,
  type = c(0.1, 0.2),
  freshness = c(0.1, 0.2),
  no_choice = -0.5  # Negative value makes no-choice less attractive
)

design_nochoice <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  no_choice = TRUE,
  priors = priors_nochoice,
  method = "stochastic"
)

head(design_nochoice)
```

## Labeled Designs

Create "labeled" or "alternative-specific" designs where one attribute serves as a label:

```{r}
design_labeled <- cbc_design(
  profiles = profiles,
  n_alts = 3,  # Will be overridden to match number of type levels
  n_q = 6,
  n_resp = 100,
  label = "type",  # Use 'type' attribute as labels
  method = "random"
)

head(design_labeled)
```

## Blocking

For D-optimal methods, create multiple design blocks to reduce respondent burden:

```{r}
design_blocked <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 200,
  n_blocks = 4,    # Create 4 different design blocks
  priors = priors,
  method = "stochastic"
)

# Check block allocation
table(design_blocked$blockID)
```

## Dominance Removal

Remove choice sets where one alternative dominates others based on parameter preferences. There are two forms of dominance removal:

1. **Total dominance**: Occurs when one alternative has such a high predicted choice probability (based on the prior coefficients) that it would be chosen by virtually all respondents. This creates choice sets with little information value since the outcome is predetermined. The `dominance_threshold` parameter controls this - alternatives with choice probabilities above this threshold (e.g., 0.8 = 80%) are considered dominant.
2. **Partial dominance**: Occurs when one alternative is superior to all others across every individual attribute component of the utility function (again, based on prior coefficients). For example, if Alternative A has higher partial utilities than Alternative B for every single attribute (price, type, freshness), then A partially dominates B regardless of the overall choice probability. This type of dominance is detected by comparing the attribute-level contributions to utility.

Both forms of dominance create unrealistic choice scenarios that provide less information about respondent preferences, so removing them generally improves design quality.

```{r}
design_no_dominance <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  priors = priors,
  method = "stochastic",
  remove_dominant = TRUE,
  dominance_types = c("total", "partial"),
  dominance_threshold = 0.8
)
```

## Interactions

Include interaction effects in D-optimal designs by specifying them in your prior model. Interactions capture how the effect of one attribute depends on the level of another attribute. The design optimization then accounts for these interaction terms when minimizing D-error.

An interaction means that the effect of one attribute changes depending on the level of another attribute. For example, consumers might be more price-sensitive for certain apple types than others. Without interactions, the model assumes:

- Price effect is constant regardless of apple type: `utility = -0.1 * price + 0.1 * type_Gala + ...`

With price × type interactions, the price effect varies by type:

- For Fuji apples: `price_effect = -0.1 + 0.05 = -0.05` (less price sensitive)
- For Gala apples: `price_effect = -0.1 + 0.02 = -0.08` (moderately price sensitive)
- For Honeycrisp apples: `price_effect = -0.1 + 0 = -0.1` (most price sensitive, reference level)

### Specifying Interaction Priors

```{r}
# Create priors with interactions
priors_interactions <- cbc_priors(
  profiles = profiles,
  price = -0.1,                   
  type = c("Fuji" = 0.1, "Gala" = 0.2), 
  freshness = c(0.1, 0.2),  
  interactions = list(
    # Price is less negative (less price sensitive) for Fuji apples
    int_spec(between = c("price", "type"), with_level = "Fuji", value = 0.05),
    # Price is slightly less negative for Gala apples
    int_spec(between = c("price", "type"), with_level = "Gala", value = 0.02)
    # Honeycrisp uses reference level (no additional interaction term)
  )
)

design_interactions <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  priors = priors_interactions,
  method = "stochastic"
)
```

When you include interactions in the prior model, the design optimization:

1. **Accounts for interaction parameters** when computing choice probabilities
2. **Optimizes profile combinations** that provide information about both main effects AND interactions
3. **Creates choice sets** that help distinguish between different interaction effects

This leads to more efficient designs when interaction effects truly exist in your population, but can reduce efficiency for estimating main effects if interactions are misspecified or don't actually exist.

### Types of Interactions Supported

```{r eval=FALSE}
# Continuous × Categorical interactions
int_spec(between = c("price", "type"), with_level = "Fuji", value = 0.05)

# Categorical × Categorical interactions
int_spec(between = c("type", "freshness"),
         level = "Fuji", with_level = "Excellent", value = 0.3)

# Continuous × Continuous interactions (if you had two continuous attributes)
int_spec(between = c("price", "weight"), value = 0.02)
```

### Important Notes

- **Only fixed parameters**: Interactions are only supported between fixed (non-random) parameters
- **Theoretical justification**: Only include interactions you have strong theoretical reasons to expect
- **Model complexity**: Each interaction adds parameters, requiring larger sample sizes for reliable estimation
- **Design efficiency**: Well-specified interactions improve design efficiency; misspecified ones can hurt it

The key insight is that by specifying interaction priors, you're telling the design algorithm to create choice sets that will be maximally informative for estimating not just main effects, but also how those effects depend on each other.

# Comparing Designs

Use `cbc_compare()` to evaluate multiple designs across key metrics:

```{r}
# Create several designs to compare
design_random_comp <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 100, method = "random")
design_shortcut_comp <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 100, method = "shortcut")
design_stochastic_comp <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 100, priors = priors, method = "stochastic")

# Compare designs
cbc_compare(
  Random = design_random_comp,
  Shortcut = design_shortcut_comp,
  Stochastic = design_stochastic_comp,
  metrics = c("efficiency", "balance", "overlap"),
  sort_by = "d_error"
)
```

# Comprehensive Design Inspection

Use `cbc_inspect()` for detailed design analysis:

```{r}
# Detailed inspection of the stochastic design
inspection <- cbc_inspect(design_stochastic, sections = "all", verbose = TRUE)
print(inspection)
```

# Design Recommendations

## For Beginners
- Start with `"random"` or `"shortcut"` methods
- Use simple designs without special features
- Focus on understanding basic design structure

## For Maximum Efficiency
- Use `"stochastic"` or `"modfed"` with good priors
- Include multiple starting points (`n_start = 5+`)
- Consider dominance removal if theoretically justified

## For Complex Models
- Use `"stochastic"` method with interaction terms
- Include no-choice options if relevant to your context
- Use blocking for large designs to reduce respondent burden

## For Restricted Profiles
- Avoid `"cea"` method (requires full factorial)
- Use `"modfed"` for best efficiency with restrictions
- Consider whether restrictions are truly necessary

# Advanced Options

## Customizing Optimization

```{r eval=FALSE}
# Advanced stochastic design with custom settings
design_advanced <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 8,
  n_resp = 300,
  n_blocks = 2,
  priors = priors,
  method = "stochastic",
  n_start = 10,           # More starting points for better optimization
  max_iter = 100,         # More iterations per start
  n_cores = 4,            # Parallel processing
  remove_dominant = TRUE,
  dominance_threshold = 0.9,
  randomize_questions = TRUE,
  randomize_alts = TRUE
)
```

## Design Verification

Always verify your design meets your requirements:

```{r}
# Check design structure and quality
design <- design_stochastic

# Basic structure
cat("Design structure:\n")
cat("- Respondents:", max(design$respID), "\n")
cat("- Questions per respondent:", max(design$qID), "\n")
cat("- Alternatives per question:", max(design$altID), "\n")
cat("- Total choice sets:", max(design$obsID), "\n")

# Check for issues
cat("\nDesign validation:\n")
cat("- Duplicate profiles in choice sets:",
    any(duplicated(paste(design$obsID, design$profileID))), "\n")
cat("- Missing profiles:", any(is.na(design$profileID)), "\n")

# Design efficiency
params <- attr(design, "design_params")
if (!is.null(params$d_error_prior)) {
  cat("- D-error (with priors):", params$d_error_prior, "\n")
}
if (!is.null(params$d_error_null)) {
  cat("- D-error (null model):", params$d_error_null, "\n")
}
```

# Next Steps

After generating your design:

1. **Inspect the design** using `cbc_inspect()` to understand its properties
2. **Simulate choices** using `cbc_choices()` to test the design
3. **Conduct power analysis** using `cbc_power()` to determine sample size requirements
4. **Compare alternatives** using `cbc_compare()` to choose the best design

For more details on these next steps, see the other vignettes in this package.