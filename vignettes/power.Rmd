---
title: "Power Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Power Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  \usepackage{xcolor}
  \usepackage{bbding}
bibliography: "`r here::here('vignettes', 'library.bib')`"
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.retina = 3,
  comment = "#>"
)
```

Power analysis determines the sample size needed to reliably detect effects of a given magnitude in your choice experiment. By simulating choice data and estimating models at different sample sizes, you can identify the minimum number of respondents needed to achieve your desired level of statistical precision. This article shows how to conduct power analyses using `cbc_power()`.

```{r}
library(cbcTools)

# Create example data for power analysis
profiles <- cbc_profiles(
  price     = c(1, 1.5, 2, 2.5, 3),
  type      = c('Fuji', 'Gala', 'Honeycrisp'),
  freshness = c('Poor', 'Average', 'Excellent')
)

# Create design and simulate choices
design <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 600,  # Large sample for power analysis
  method = "random"
)

priors <- cbc_priors(
  profiles = profiles,
  price = -0.1,
  type = c(0.1, 0.2),
  freshness = c(0.1, 0.2)
)

choices <- cbc_choices(design, priors = priors)
head(choices)
```

# Understanding Power Analysis

## What is Statistical Power?

Statistical power is the probability of correctly detecting an effect when it truly exists. In choice experiments, power depends on:

- **Effect size**: Larger effects are easier to detect
- **Sample size**: More respondents provide more precision
- **Design efficiency**: Better designs extract more information per respondent
- **Model complexity**: More parameters require larger samples

## Why Conduct Power Analysis?

- **Sample size planning**: Determine minimum respondents needed
- **Budget planning**: Estimate data collection costs
- **Design comparison**: Choose between alternative experimental designs
- **Feasibility assessment**: Check if research questions are answerable with available resources

## Power vs. Precision

Power analysis in `cbc_power()` focuses on **precision** (standard errors) rather than traditional hypothesis testing power, because:

- Provides more actionable information for sample size planning
- Relevant for both significant and non-significant results
- Easier to interpret across different effect sizes
- More directly tied to practical research needs

# Basic Power Analysis

## Simple Power Analysis

Start with a basic power analysis using auto-detection of parameters:

```{r}
# Basic power analysis with auto-detected parameters
power_basic <- cbc_power(
  data = choices,
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 10
)

# View the power analysis object
power_basic

# Access the detailed results data frame
head(power_basic$power_summary)
tail(power_basic$power_summary)
```

## Parameter Specification Options

### Auto-Detection (Recommended)

By default, `cbc_power()` automatically detects all attribute parameters from your choice data:

```{r}
# Auto-detection works with dummy-coded data
power_auto <- cbc_power(
  data = choices,
  outcome = "choice", 
  obsID = "obsID",
  n_q = 6,
  n_breaks = 8
)

# Shows all parameters: price, typeGala, typeHoneycrisp, freshnessAverage, freshnessExcellent
```

### Specify Dummy-Coded Parameters

You can explicitly specify which dummy-coded parameters to include:

```{r}
# Focus on specific dummy-coded parameters
power_specific <- cbc_power(
  data = choices,
  pars = c("price", "typeHoneycrisp", "freshnessExcellent"),  # Specific dummy variables
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 8
)
```

### Use Decoded Data with Attribute Names

For easier interpretation, decode the choice data first to use original attribute names:

```{r}
# Decode choice data to get back categorical variables
choices_decoded <- cbc_decode(choices)

# Now you can use attribute names instead of dummy variables
power_decoded <- cbc_power(
  data = choices_decoded,
  pars = c("price", "type", "freshness"),  # Original attribute names
  outcome = "choice",
  obsID = "obsID", 
  n_q = 6,
  n_breaks = 8
)

# Note: This approach estimates effects differently - 
# it treats categorical variables as factors rather than separate dummy variables
```

### When to Use Each Approach

- **Auto-detection**: Best for comprehensive power analysis of all effects
- **Dummy-coded specification**: When you want to focus on specific levels of categorical variables
- **Decoded data**: When you want power analysis at the attribute level rather than level-specific effects, or for easier interpretation

```{r}
# Compare the different approaches
cat("Auto-detected parameters:\n")
unique_params_auto <- unique(power_auto$power_summary$parameter)
cat(paste(unique_params_auto, collapse = ", "), "\n\n")

cat("Specific dummy parameters:\n") 
unique_params_specific <- unique(power_specific$power_summary$parameter)
cat(paste(unique_params_specific, collapse = ", "), "\n\n")

cat("Decoded attribute parameters:\n")
unique_params_decoded <- unique(power_decoded$power_summary$parameter)
cat(paste(unique_params_decoded, collapse = ", "), "\n")
```

## Understanding Power Results

The power analysis returns a list object with several components:

- **`power_summary`**: Data frame with sample sizes, coefficients, estimates, standard errors, t-statistics, and power
- **`sample_sizes`**: Vector of sample sizes tested  
- **`n_breaks`**: Number of breaks used
- **`alpha`**: Significance level used
- **`choice_info`**: Information about the underlying choice simulation

The `power_summary` data frame contains:

- **sample_size**: Number of respondents in each analysis
- **parameter**: Parameter name being estimated
- **estimate**: Coefficient estimate
- **std_error**: Standard error of the estimate
- **t_statistic**: t-statistic (estimate/std_error)
- **power**: Statistical power (probability of detecting effect)

## Visualizing Power Curves

Plot power curves to visualize the relationship between sample size and precision:

```{r}
# Plot standard error curves
plot(power_basic, type = "se")

# Plot power curves  
plot(power_basic, type = "power", power_threshold = 0.8)
```

## Interpreting Results

```{r}
# Sample size requirements for 80% power
summary(power_basic, power_threshold = 0.8)

# You can also access specific components
cat("Sample sizes tested:", paste(power_basic$sample_sizes, collapse = ", "), "\n")
cat("Number of breaks:", power_basic$n_breaks, "\n")
cat("Alpha level:", power_basic$alpha, "\n")
```

From these results, you can determine:
- Which parameters need the largest samples
- Whether your planned sample size is adequate
- How much precision improves with additional respondents

# Advanced Power Analysis Options

## Custom Parameter Selection

Focus power analysis on specific parameters of interest using any of the approaches:

```{r}
# Power analysis for price coefficient only (dummy-coded)
power_price <- cbc_power(
  data = choices,
  pars = "price",  # Focus on price parameter
  outcome = "choice",
  obsID = "obsID", 
  n_q = 6,
  n_breaks = 8
)

# Power analysis for categorical attributes only (using decoded data)
choices_decoded <- cbc_decode(choices)
power_categorical <- cbc_power(
  data = choices_decoded,
  pars = c("type", "freshness"),  # Attribute-level analysis
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 8
)
```

## Different Break Points

Control the sample size increments tested:

```{r}
# More detailed power curve with more break points
power_detailed <- cbc_power(
  data = choices,
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 20  # More granular analysis
)

# Focus on smaller sample sizes
small_sample_choices <- choices[choices$respID <= 200, ]
power_small <- cbc_power(
  data = small_sample_choices,
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 10
)
```

## Mixed Logit Models

Conduct power analysis for random parameter models:

```{r}
# Create choices with random parameters
priors_random <- cbc_priors(
  profiles = profiles,
  price = rand_spec(dist = "n", mean = -0.1, sd = 0.05),
  type = rand_spec(dist = "n", mean = c(0.1, 0.2), sd = c(0.05, 0.1)),
  freshness = c(0.1, 0.2)
)

choices_mixed <- cbc_choices(design, priors = priors_random)

# Power analysis for mixed logit model
power_mixed <- cbc_power(
  data = cbc_decode(choices_mixed),
  pars = c("price", "type", "freshness"),
  randPars = c(price = "n", type = "n"),  # Specify random parameters
  outcome = "choice",
  obsID = "obsID",
  panelID = "respID",  # Required for panel data
  n_q = 6,
  n_breaks = 10
)

# Mixed logit models generally require larger samples
power_mixed
```

# Comparing Design Performance

## Design Method Comparison

Compare power across different design methods:

```{r}
# Create designs with different methods
design_random <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400, method = "random")
design_shortcut <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400, method = "shortcut")
design_optimal <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400, 
                            priors = priors, method = "stochastic")

# Simulate choices with same priors for fair comparison
choices_random <- cbc_choices(design_random, priors = priors)
choices_shortcut <- cbc_choices(design_shortcut, priors = priors)
choices_optimal <- cbc_choices(design_optimal, priors = priors)

# Conduct power analysis for each
power_random <- cbc_power(
  choices_random, 
  outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8
)
power_shortcut <- cbc_power(
  choices_shortcut, 
  outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8
)
power_optimal <- cbc_power(
  choices_optimal, 
  outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8
)

# Compare power curves
plot_compare_power(
  Random = power_random,
  Shortcut = power_shortcut,
  Optimal = power_optimal,
  type = "power"
)
```

## Design Feature Comparison

Test impact of design features on power:

```{r}
# Compare standard design vs design with no-choice
design_standard <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400)

design_nochoice <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400, no_choice = TRUE)

priors_nochoice <- cbc_priors(profiles, price = -0.1, type = c(0.1, 0.2), 
                             freshness = c(0.1, 0.2), no_choice = -0.5)

choices_standard <- cbc_choices(design_standard, priors = priors)
choices_nochoice <- cbc_choices(design_nochoice, priors = priors_nochoice)

# Compare power
power_standard <- cbc_power(choices_standard, 
                           outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)
power_nochoice <- cbc_power(choices_nochoice, 
                           outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)

# No-choice typically reduces power for main effects
plot_compare_power(
  Standard = power_standard,
  `No Choice` = power_nochoice,
  type = "se"
)
```

# Advanced Analysis

## Returning Full Models

Access complete model objects for detailed analysis:

```{r}
# Return full models for additional analysis
power_with_models <- cbc_power(
  data = choices,
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 5,
  return_models = TRUE
)

# Examine largest model
largest_model <- power_with_models$models[[length(power_with_models$models)]]
summary(largest_model)
```


# Best Practices

## Power Analysis Workflow

1. **Start with literature**: Base effect size assumptions on previous studies
2. **Use realistic priors**: Conservative estimates are often better than optimistic ones
3. **Test multiple scenarios**: Conservative, moderate, and optimistic effect sizes
4. **Compare designs**: Test different design methods and features
5. **Plan for attrition**: Add 10-20% to account for incomplete responses
6. **Document assumptions**: Record all assumptions for future reference

## Common Pitfalls

```{r eval=FALSE}
# Pitfall 1: Unrealistic effect sizes
# Too large: priors = list(price = -2.0)  # Unrealistically strong
# Too small: priors = list(price = -0.001)  # Undetectable

# Pitfall 2: Inconsistent priors between design and power analysis
# Use same priors for design optimization and power analysis

# Pitfall 3: Ignoring design method impact
# D-optimal designs can substantially reduce required sample sizes

# Pitfall 4: Overly optimistic planning
# Include pessimistic scenarios in your analysis

# Pitfall 5: Forgetting about attrition
# Plan for 10-20% incomplete responses
```

## Sample Size Recommendations

General guidance for different study types:

```{r}
# Sample size recommendations
recommendations <- data.frame(
  Study_Type = c(
    "Pilot/Exploratory",
    "Academic Research", 
    "Market Research",
    "Policy Analysis",
    "Product Development"
  ),
  Min_Sample = c(50, 200, 300, 500, 200),
  Typical_Range = c(
    "50-150",
    "200-800",
    "300-1000", 
    "500-2000",
    "200-600"
  ),
  Notes = c(
    "Focus on feasibility",
    "Balance power and budget",
    "Commercial precision needs",
    "High stakes decisions",
    "Iterative testing"
  )
)

print(recommendations)
```

# Validation and Reporting

## Validating Power Analysis

Check that your power analysis makes sense:

```{r}
# Validation checks
validate_power <- function(power_object) {
  power_data <- power_object$power_summary
  
  # 1. Standard errors should decrease with sample size
  param_data <- power_data[power_data$parameter == "price", ]
  param_data <- param_data[order(param_data$sample_size), ]
  se_decreasing <- all(diff(param_data$std_error) <= 0)
  
  # 2. Power should increase with sample size (when power < 1)
  power_increasing <- all(diff(param_data$power) >= 0)
  
  # 3. Estimates should be relatively stable
  est_stability <- sd(param_data$estimate) / mean(abs(param_data$estimate))
  
  cat("Power analysis validation:\n")
  cat("- Standard errors decrease with sample size:", se_decreasing, "\n")
  cat("- Power increases with sample size:", power_increasing, "\n")
  cat("- Estimate stability (CV):", round(est_stability, 3), "\n")
  
  if (!se_decreasing || !power_increasing) {
    warning("Power analysis may have issues - check data and parameters")
  }
}

validate_power(power_basic)
```

## Reporting Power Analysis

Structure for reporting power analysis results:

```{r eval=FALSE}
# Example reporting structure:

# 1. Methods
"Power analysis was conducted using simulated choice data based on 
[source of priors]. Choice data were simulated using a multinomial logit 
model with [describe priors]. Sample sizes from X to Y respondents were 
tested in Z increments."

# 2. Results  
"For 80% power to detect the assumed effect sizes, the following sample 
sizes are required: [parameter]: N respondents, [parameter]: N respondents."

# 3. Recommendations
"Based on the power analysis and budget constraints, we recommend a sample 
size of N respondents, which provides [power level]% power for detecting 
the smallest effect of interest."

# 4. Sensitivity
"Sensitivity analysis showed that if true effect sizes are [X]% smaller 
than assumed, the required sample size would increase to N respondents."
```

# Next Steps

After completing power analysis:

1. **Finalize sample size**: Choose based on power requirements and budget
2. **Plan data collection**: Account for attrition and incomplete responses  
3. **Document assumptions**: Record all priors and assumptions used
4. **Prepare analysis plan**: Pre-specify models and analysis approach
5. **Consider adaptive approaches**: Plan interim analyses if appropriate

For a complete workflow example, see the "Getting Started" vignette that ties together all steps from profiles to power analysis.