---
title: "Power Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Power Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  \usepackage{xcolor}
  \usepackage{bbding}
bibliography: "`r here::here('vignettes', 'library.bib')`"
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.retina = 3,
  comment = "#>"
)
```

Power analysis determines the sample size needed to reliably detect effects of a given magnitude in your choice experiment. By simulating choice data and estimating models at different sample sizes, you can identify the minimum number of respondents needed to achieve your desired level of statistical precision. This article shows how to conduct power analyses using `cbc_power()`.

```{r}
library(cbcTools)

# Create example data for power analysis
profiles <- cbc_profiles(
  price     = c(1, 1.5, 2, 2.5, 3),
  type      = c('Fuji', 'Gala', 'Honeycrisp'),
  freshness = c('Poor', 'Average', 'Excellent')
)

# Create design and simulate choices
design <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 600,  # Large sample for power analysis
  method = "random"
)

priors <- cbc_priors(
  profiles = profiles,
  price = -0.1,
  type = c(0.1, 0.2),
  freshness = c(0.1, 0.2)
)

choices <- cbc_choices(design, priors = priors)
head(choices)
```

# Understanding Power Analysis

## What is Statistical Power?

Statistical power is the probability of correctly detecting an effect when it truly exists. In choice experiments, power depends on:

- **Effect size**: Larger effects are easier to detect
- **Sample size**: More respondents provide more precision
- **Design efficiency**: Better designs extract more information per respondent
- **Model complexity**: More parameters require larger samples

## Why Conduct Power Analysis?

- **Sample size planning**: Determine minimum respondents needed
- **Budget planning**: Estimate data collection costs
- **Design comparison**: Choose between alternative experimental designs
- **Feasibility assessment**: Check if research questions are answerable with available resources

## Power vs. Precision

Power analysis in `cbc_power()` focuses on **precision** (standard errors) rather than traditional hypothesis testing power, because:

- Provides more actionable information for sample size planning
- Relevant for both significant and non-significant results
- Easier to interpret across different effect sizes
- More directly tied to practical research needs

# Basic Power Analysis

## Simple Power Analysis

Start with a basic power analysis using default settings:

```{r}
# Basic power analysis
power_basic <- cbc_power(
  data = choices,
  pars = c("price", "type", "freshness"),
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 10
)

head(power_basic)
tail(power_basic)
```

## Understanding Power Results

The power analysis returns a data frame with:

- **sample_size**: Number of respondents in each analysis
- **parameter**: Parameter name being estimated
- **estimate**: Coefficient estimate
- **std_error**: Standard error of the estimate
- **t_statistic**: t-statistic (estimate/std_error)
- **power**: Statistical power (probability of detecting effect)

## Visualizing Power Curves

Plot power curves to visualize the relationship between sample size and precision:

```{r}
# Plot standard error curves
plot(power_basic, type = "se")

# Plot power curves
plot(power_basic, type = "power", power_threshold = 0.8)
```

## Interpreting Results

```{r}
# Sample size requirements for 80% power
summary(power_basic, power_threshold = 0.8)
```

From these results, you can determine:
- Which parameters need the largest samples
- Whether your planned sample size is adequate
- How much precision improves with additional respondents

# Advanced Power Analysis Options

## Custom Parameter Selection

Focus power analysis on specific parameters of interest:

```{r}
# Power analysis for price coefficient only
power_price <- cbc_power(
  data = choices,
  pars = "price",  # Focus on price parameter
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 8
)

# Power analysis for categorical attributes only
power_categorical <- cbc_power(
  data = choices,
  pars = c("type", "freshness"),
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 8
)
```

## Different Break Points

Control the sample size increments tested:

```{r}
# More detailed power curve with more break points
power_detailed <- cbc_power(
  data = choices,
  pars = c("price", "type", "freshness"),
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 20  # More granular analysis
)

# Focus on smaller sample sizes
small_sample_choices <- choices[choices$respID <= 200, ]
power_small <- cbc_power(
  data = small_sample_choices,
  pars = c("price", "type", "freshness"),
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 10
)
```

## Mixed Logit Models

Conduct power analysis for random parameter models:

```{r}
# Create choices with random parameters
priors_random <- cbc_priors(
  profiles = profiles,
  price = rand_spec(dist = "n", mean = -0.1, sd = 0.05),
  type = rand_spec(dist = "n", mean = c(0.1, 0.2), sd = c(0.05, 0.1)),
  freshness = c(0.1, 0.2)
)

choices_mixed <- cbc_choices(design, priors = priors_random)

# Power analysis for mixed logit model
power_mixed <- cbc_power(
  data = choices_mixed,
  pars = c("price", "type", "freshness"),
  randPars = c(price = "n", type = "n"),  # Specify random parameters
  outcome = "choice",
  obsID = "obsID",
  panelID = "respID",  # Required for panel data
  n_q = 6,
  n_breaks = 10
)

# Mixed logit models generally require larger samples
summary(power_mixed, power_threshold = 0.8)
```

# Comparing Design Performance

## Design Method Comparison

Compare power across different design methods:

```{r}
# Create designs with different methods
design_random <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400, method = "random")
design_shortcut <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400, method = "shortcut")
design_optimal <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400,
                            priors = priors, method = "stochastic")

# Simulate choices with same priors for fair comparison
choices_random <- cbc_choices(design_random, priors = priors)
choices_shortcut <- cbc_choices(design_shortcut, priors = priors)
choices_optimal <- cbc_choices(design_optimal, priors = priors)

# Conduct power analysis for each
power_random <- cbc_power(choices_random, pars = c("price", "type", "freshness"),
                         outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)
power_shortcut <- cbc_power(choices_shortcut, pars = c("price", "type", "freshness"),
                           outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)
power_optimal <- cbc_power(choices_optimal, pars = c("price", "type", "freshness"),
                          outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)

# Compare power curves
plot_compare_power(
  Random = power_random,
  Shortcut = power_shortcut,
  Optimal = power_optimal,
  type = "power"
)
```

## Design Feature Comparison

Test impact of design features on power:

```{r}
# Compare standard design vs design with no-choice
design_standard <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400)

design_nochoice <- cbc_design(profiles, n_alts = 2, n_q = 6, n_resp = 400, no_choice = TRUE)

priors_nochoice <- cbc_priors(profiles, price = -0.1, type = c(0.1, 0.2),
                             freshness = c(0.1, 0.2), no_choice = -0.5)

choices_standard <- cbc_choices(design_standard, priors = priors)
choices_nochoice <- cbc_choices(design_nochoice, priors = priors_nochoice)

# Compare power
power_standard <- cbc_power(choices_standard, pars = c("price", "type", "freshness"),
                           outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)
power_nochoice <- cbc_power(choices_nochoice, pars = c("price", "type", "freshness"),
                           outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)

# No-choice typically reduces power for main effects
plot_compare_power(
  Standard = power_standard,
  `No Choice` = power_nochoice,
  type = "se"
)
```

# Practical Applications

## Sample Size Planning

Use power analysis for sample size determination:

```{r}
# Determine sample size for different precision targets
power_results <- power_basic

# Function to find required sample size for target standard error
find_sample_size <- function(power_data, parameter, target_se) {
  param_data <- power_data[power_data$parameter == parameter, ]
  param_data <- param_data[order(param_data$sample_size), ]

  # Find first sample size that achieves target
  adequate <- param_data$std_error <= target_se
  if (any(adequate)) {
    return(min(param_data$sample_size[adequate]))
  } else {
    return(paste("Requires >", max(param_data$sample_size)))
  }
}

# Sample size requirements for SE = 0.02
cat("Sample sizes needed for standard error â‰¤ 0.02:\n")
cat("Price:", find_sample_size(power_results, "price", 0.02), "\n")
cat("Type (Gala):", find_sample_size(power_results, "typeGala", 0.02), "\n")
cat("Type (Honeycrisp):", find_sample_size(power_results, "typeHoneycrisp", 0.02), "\n")
cat("Freshness (Average):", find_sample_size(power_results, "freshnessAverage", 0.02), "\n")
cat("Freshness (Excellent):", find_sample_size(power_results, "freshnessExcellent", 0.02), "\n")
```

## Budget Planning

Translate sample size requirements into cost estimates:

```{r}
# Hypothetical cost structure
cost_per_respondent <- 5  # $5 per complete response
fixed_costs <- 2000       # $2000 setup costs

calculate_budget <- function(n_respondents) {
  total_cost <- fixed_costs + (n_respondents * cost_per_respondent)
  return(total_cost)
}

# Budget estimates for different sample sizes
sample_sizes <- c(100, 200, 300, 400, 500, 600)
budgets <- sapply(sample_sizes, calculate_budget)

budget_table <- data.frame(
  sample_size = sample_sizes,
  budget = budgets,
  budget_formatted = paste("$", format(budgets, big.mark = ","), sep = "")
)

print(budget_table)
```

## Scenario Analysis

Test power under different assumption scenarios:

```{r}
# Scenario 1: Conservative effect sizes
priors_conservative <- cbc_priors(profiles, price = -0.05, type = c(0.05, 0.1), freshness = c(0.05, 0.1))
choices_conservative <- cbc_choices(design, priors = priors_conservative)
power_conservative <- cbc_power(choices_conservative, pars = c("price", "type", "freshness"),
                               outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)

# Scenario 2: Optimistic effect sizes
priors_optimistic <- cbc_priors(profiles, price = -0.2, type = c(0.2, 0.4), freshness = c(0.2, 0.4))
choices_optimistic <- cbc_choices(design, priors = priors_optimistic)
power_optimistic <- cbc_power(choices_optimistic, pars = c("price", "type", "freshness"),
                             outcome = "choice", obsID = "obsID", n_q = 6, n_breaks = 8)

# Compare scenarios
plot_compare_power(
  Conservative = power_conservative,
  Baseline = power_basic,
  Optimistic = power_optimistic,
  type = "power"
)
```

# Advanced Analysis

## Complex Models

Power analysis for models with interactions:

```{r eval=FALSE}
# Create data with interaction effects
priors_interactions <- cbc_priors(
  profiles = profiles,
  price = -0.1,
  type = c(0.1, 0.2),
  freshness = c(0.1, 0.2),
  interactions = list(
    int_spec(between = c("price", "type"), with_level = "Fuji", value = 0.05)
  )
)

choices_interactions <- cbc_choices(design, priors = priors_interactions)

# Power analysis including interaction terms
power_interactions <- cbc_power(
  data = choices_interactions,
  pars = c("price", "type", "freshness", "price:typeFuji"),  # Include interaction
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 10
)

# Interaction terms typically require larger samples
```

## Returning Full Models

Access complete model objects for detailed analysis:

```{r}
# Return full models for additional analysis
power_with_models <- cbc_power(
  data = choices,
  pars = c("price", "type", "freshness"),
  outcome = "choice",
  obsID = "obsID",
  n_q = 6,
  n_breaks = 5,
  return_models = TRUE
)

# Examine largest model
largest_model <- power_with_models$models[[length(power_with_models$models)]]
summary(largest_model)

# Extract additional model statistics
cat("Log-likelihood:", logLik(largest_model), "\n")
cat("AIC:", AIC(largest_model), "\n")
```

# Interpretation Guidelines

## Standard Error Benchmarks

Common standard error targets for different research goals:

```{r}
# Interpretation guidelines
interpretation_guide <- data.frame(
  Research_Goal = c("Exploratory", "Confirmatory", "High Precision", "Publication"),
  Target_SE = c("0.05-0.10", "0.03-0.05", "0.01-0.03", "0.02-0.04"),
  Description = c(
    "Initial exploration of effects",
    "Testing specific hypotheses",
    "Precise effect estimation",
    "Academic publication standards"
  )
)

print(interpretation_guide)
```

## Power Thresholds

Standard power levels and their interpretations:

```{r}
# Power interpretation
power_guide <- data.frame(
  Power_Level = c("50%", "80%", "90%", "95%"),
  Interpretation = c(
    "Coin flip chance of detection",
    "Standard social science threshold",
    "High power for critical studies",
    "Very high power (often overkill)"
  ),
  Use_Case = c(
    "Not recommended",
    "Most research applications",
    "Important policy decisions",
    "Safety-critical applications"
  )
)

print(power_guide)
```

## Effect Size Context

Put your power analysis in context of typical effect sizes:

```{r}
# Typical effect sizes in choice experiments
effect_guide <- data.frame(
  Effect_Type = c("Price", "Important Attribute", "Moderate Attribute", "Weak Attribute"),
  Typical_Range = c("-0.1 to -0.5", "0.2 to 0.8", "0.05 to 0.2", "0.01 to 0.05"),
  Examples = c(
    "Price per unit",
    "Brand, quality levels",
    "Secondary features",
    "Minor product details"
  )
)

print(effect_guide)
```

# Best Practices

## Power Analysis Workflow

1. **Start with literature**: Base effect size assumptions on previous studies
2. **Use realistic priors**: Conservative estimates are often better than optimistic ones
3. **Test multiple scenarios**: Conservative, moderate, and optimistic effect sizes
4. **Compare designs**: Test different design methods and features
5. **Plan for attrition**: Add 10-20% to account for incomplete responses
6. **Document assumptions**: Record all assumptions for future reference

## Common Pitfalls

```{r eval=FALSE}
# Pitfall 1: Unrealistic effect sizes
# Too large: priors = list(price = -2.0)  # Unrealistically strong
# Too small: priors = list(price = -0.001)  # Undetectable

# Pitfall 2: Inconsistent priors between design and power analysis
# Use same priors for design optimization and power analysis

# Pitfall 3: Ignoring design method impact
# D-optimal designs can substantially reduce required sample sizes

# Pitfall 4: Overly optimistic planning
# Include pessimistic scenarios in your analysis

# Pitfall 5: Forgetting about attrition
# Plan for 10-20% incomplete responses
```

## Sample Size Recommendations

General guidance for different study types:

```{r}
# Sample size recommendations
recommendations <- data.frame(
  Study_Type = c(
    "Pilot/Exploratory",
    "Academic Research",
    "Market Research",
    "Policy Analysis",
    "Product Development"
  ),
  Min_Sample = c(50, 200, 300, 500, 200),
  Typical_Range = c(
    "50-150",
    "200-800",
    "300-1000",
    "500-2000",
    "200-600"
  ),
  Notes = c(
    "Focus on feasibility",
    "Balance power and budget",
    "Commercial precision needs",
    "High stakes decisions",
    "Iterative testing"
  )
)

print(recommendations)
```

# Validation and Reporting

## Validating Power Analysis

Check that your power analysis makes sense:

```{r}
# Validation checks
validate_power <- function(power_data) {

  # 1. Standard errors should decrease with sample size
  param_data <- power_data[power_data$parameter == "price", ]
  se_decreasing <- all(diff(param_data$std_error) <= 0)

  # 2. Power should increase with sample size (when power < 1)
  power_increasing <- all(diff(param_data$power) >= 0)

  # 3. Estimates should be relatively stable
  est_stability <- sd(param_data$estimate) / mean(abs(param_data$estimate))

  cat("Power analysis validation:\n")
  cat("- Standard errors decrease with sample size:", se_decreasing, "\n")
  cat("- Power increases with sample size:", power_increasing, "\n")
  cat("- Estimate stability (CV):", round(est_stability, 3), "\n")

  if (!se_decreasing || !power_increasing) {
    warning("Power analysis may have issues - check data and parameters")
  }
}

validate_power(power_basic)
```

## Reporting Power Analysis

Structure for reporting power analysis results:

```{r eval=FALSE}
# Example reporting structure:

# 1. Methods
"Power analysis was conducted using simulated choice data based on
[source of priors]. Choice data were simulated using a multinomial logit
model with [describe priors]. Sample sizes from X to Y respondents were
tested in Z increments."

# 2. Results
"For 80% power to detect the assumed effect sizes, the following sample
sizes are required: [parameter]: N respondents, [parameter]: N respondents."

# 3. Recommendations
"Based on the power analysis and budget constraints, we recommend a sample
size of N respondents, which provides [power level]% power for detecting
the smallest effect of interest."

# 4. Sensitivity
"Sensitivity analysis showed that if true effect sizes are [X]% smaller
than assumed, the required sample size would increase to N respondents."
```

# Next Steps

After completing power analysis:

1. **Finalize sample size**: Choose based on power requirements and budget
2. **Plan data collection**: Account for attrition and incomplete responses
3. **Document assumptions**: Record all priors and assumptions used
4. **Prepare analysis plan**: Pre-specify models and analysis approach
5. **Consider adaptive approaches**: Plan interim analyses if appropriate

For a complete workflow example, see the "Getting Started" vignette that ties together all steps from profiles to power analysis.