---
title: "Simulating Choices"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simulating Choices}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  \usepackage{xcolor}
  \usepackage{bbding}
bibliography: "`r here::here('vignettes', 'library.bib')`"
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.retina = 3,
  comment = "#>"
)
```

Choice simulation converts experimental designs into realistic choice data by predicting how respondents would answer choice questions. This is essential for testing designs, conducting power analyses, and validating experimental assumptions before data collection. This article shows how to use `cbc_choices()` to simulate choice patterns.

```{r}
library(cbcTools)

# Create example profiles and design
profiles <- cbc_profiles(
  price     = c(1, 1.5, 2, 2.5, 3),
  type      = c('Fuji', 'Gala', 'Honeycrisp'),
  freshness = c('Poor', 'Average', 'Excellent')
)

design <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  method = "random"
)

head(design)
```

# Understanding Choice Simulation

## What is Choice Simulation?

Choice simulation predicts which alternative a respondent would choose in each choice question based on:

- **Design structure**: The alternatives presented in each choice set
- **Preference assumptions**: How attributes influence utility (specified through priors)
- **Random variation**: Stochastic choice behavior captured by error terms

## Simulation Methods

`cbc_choices()` supports two simulation approaches:

1. **Random simulation**: Each alternative has equal probability of being chosen
2. **Utility-based simulation**: Choice probabilities based on multinomial logit model with specified priors

## Why Simulate Choices?

- **Power analysis**: Determine sample size requirements before data collection
- **Design testing**: Evaluate how well designs perform under different scenarios
- **Method validation**: Test analysis approaches on data with known parameters
- **Assumption checking**: Verify that design and priors produce reasonable choice patterns

# Basic Choice Simulation

## Random Choices

Without priors, choices are simulated randomly with equal probabilities:

```{r}
# Random choice simulation (default)
choices_random <- cbc_choices(design)

head(choices_random)

# Check choice distribution
table(choices_random$choice, choices_random$altID)
```

Random simulation is useful for:
- Quick testing of design structure
- Conservative power analysis (worst-case scenario)
- Baseline comparisons

## Utility-Based Choices

With priors, choices follow realistic utility-based patterns:

```{r}
# Create priors for utility-based simulation
priors <- cbc_priors(
  profiles = profiles,
  price = -0.1,           # Negative preference for higher prices
  type = c(0.5, 1),     # Gala and Honeycrisp preferred over Fuji
  freshness = c(0.5, 1) # Average and Excellent preferred over Poor
)

# Utility-based choice simulation
choices_utility <- cbc_choices(design, priors = priors)

head(choices_utility)

# Compare choice patterns
cat("Random simulation choice rates by alternative:\n")
random_rates <- prop.table(table(choices_random$altID, choices_random$choice), 1)
print(round(random_rates, 3))

cat("\nUtility-based simulation choice rates by alternative:\n")
utility_rates <- prop.table(table(choices_utility$altID, choices_utility$choice), 1)
print(round(utility_rates, 3))
```

# Understanding Choice Data Structure

## Choice Data Format

The simulated choice data includes all design columns plus a `choice` column:

```{r}
# Examine the structure  
head(choices_utility)

# Key columns:
# - profileID: Links to original profiles
# - respID, qID, altID, obsID: Design structure
# - choice: 1 if chosen, 0 if not chosen
# - Attribute columns: Dummy-coded attribute levels
```

## Choice Verification

Verify that exactly one alternative is chosen per question:

```{r}
# Check that each question has exactly one choice
choices_per_question <- aggregate(choice ~ obsID, data = choices_utility, sum)
table(choices_per_question$choice)

# Should be all 1s (one choice per question)
all(choices_per_question$choice == 1)
```

## Response Patterns

Examine individual respondent patterns using decoded data:

```{r}
# Decode the choice data first to get categorical variables
choices_decoded <- cbc_decode(choices_utility)

# Choices for first respondent
resp1_choices <- choices_decoded[choices_decoded$respID == 1, ]
resp1_summary <- resp1_choices[resp1_choices$choice == 1, ]
print(resp1_summary)

# Count choices by attribute levels
cat("\nAttribute choices for respondent 1:\n")
cat("Price choices:", table(resp1_summary$price), "\n")
cat("Type choices:", table(resp1_summary$type), "\n")
cat("Freshness choices:", table(resp1_summary$freshness), "\n")
```

# Advanced Simulation Options

## Designs with No-Choice

For designs with no-choice options, specify no-choice priors:

```{r}
# Create design with no-choice option
design_nochoice <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  no_choice = TRUE,
  method = "random"
)

# Create priors including no-choice utility
priors_nochoice <- cbc_priors(
  profiles = profiles,
  price = -0.1,
  type = c(0.1, 0.2),
  freshness = c(0.1, 0.2),
  no_choice = -0.5  # Negative = no-choice less attractive
)

# Simulate choices
choices_nochoice <- cbc_choices(design_nochoice, priors = priors_nochoice)

# Examine no-choice rates
nochoice_rate <- mean(choices_nochoice$choice[choices_nochoice$no_choice == 1])
cat("No-choice selection rate:", round(nochoice_rate * 100, 1), "%\n")
```

## Random Parameters (Mixed Logit)

Simulate heterogeneous preferences using random parameters:

```{r}
# Create priors with random parameters
priors_random <- cbc_priors(
  profiles = profiles,
  price = rand_spec(dist = "n", mean = -0.1, sd = 0.05),
  type = rand_spec(dist = "n", mean = c(0.1, 0.2), sd = c(0.05, 0.1)),
  freshness = c(0.1, 0.2),  # Keep some parameters fixed
  n_draws = 100
)

# Simulate choices with preference heterogeneity
choices_mixed <- cbc_choices(design, priors = priors_random)

# The choice patterns will vary more across respondents
# due to individual-level preference heterogeneity
```

## Interaction Effects

Include interaction effects in choice simulation:

```{r}
# Create priors with interactions
priors_interactions <- cbc_priors(
  profiles = profiles,
  price = -0.1,
  type = c("Fuji" = 0.5, "Gala" = 1),
  freshness = c("Average" = 0.5, "Excellent" = 1),
  interactions = list(
    # Price sensitivity varies by apple type
    int_spec(between = c("price", "type"), with_level = "Fuji", value = 0.05),
    int_spec(between = c("price", "type"), with_level = "Gala", value = 0.02)
  )
)

# Simulate choices with interaction effects
choices_interactions <- cbc_choices(design, priors = priors_interactions)

# Interaction effects create more complex choice patterns
# Price sensitivity differs across apple types
```

# Validating Choice Patterns

## Overall Choice Frequencies

Examine aggregate choice patterns to validate simulation:

```{r}
# Decode the choice data first to get categorical variables
choices_decoded <- cbc_decode(choices_utility)

# Aggregate attribute choices across all respondents
choices <- choices_decoded

# Price choices
price_choices <- aggregate(choice ~ price, data = choices, sum)
price_choices$prop <- price_choices$choice / sum(price_choices$choice)
print(price_choices)

# Type choices  
type_choices <- aggregate(choice ~ type, data = choices, sum)
type_choices$prop <- type_choices$choice / sum(type_choices$choice)
print(type_choices)

# Freshness choices
freshness_choices <- aggregate(choice ~ freshness, data = choices, sum)
freshness_choices$prop <- freshness_choices$choice / sum(freshness_choices$choice)
print(freshness_choices)
```

## Expected Patterns

Based on the priors used, we expect:
- **Lower prices preferred** (negative price coefficient)
- **Honeycrisp > Gala > Fuji** (type coefficients: 0.2 > 0.1 > 0)
- **Excellent > Average > Poor** (freshness coefficients: 0.2 > 0.1 > 0)

## Respondent Heterogeneity

For random parameter models, examine variation across respondents:

```{r}
# Create dataset with only chosen alternatives
chosen_alts <- choices_mixed[choices_mixed$choice == 1, ]

# Mean attribute levels chosen by each respondent
resp_means <- aggregate(cbind(price, typeGala, typeHoneycrisp, freshnessAverage, freshnessExcellent) ~ respID, 
                       data = chosen_alts, mean)

# Look at variation across respondents
cat("Price variation across respondents:\n")
cat("Mean:", round(mean(resp_means$price), 2), "\n")
cat("SD:", round(sd(resp_means$price), 2), "\n")

cat("\nHoneycrisp choice rate variation:\n")
cat("Mean:", round(mean(resp_means$typeHoneycrisp), 2), "\n")
cat("SD:", round(sd(resp_means$typeHoneycrisp), 2), "\n")
```

# Design Consistency

## Using Consistent Priors

For D-optimal designs created with priors, use the same priors for choice simulation:

```{r}
# Create D-optimal design with priors
design_optimal <- cbc_design(
  profiles = profiles,
  n_alts = 2,
  n_q = 6,
  n_resp = 100,
  priors = priors,
  method = "stochastic"
)

# Use SAME priors for choice simulation
choices_consistent <- cbc_choices(design_optimal, priors = priors)

# This ensures the simulated choices match the assumptions
# used during design optimization
```

## Prior Consistency Warnings

cbcTools warns when different priors are used:

```{r}
# Create different priors
different_priors <- cbc_priors(
  profiles = profiles,
  price = -0.2,  # Different from design optimization
  type = c(0.2, 0.4),
  freshness = c(0.2, 0.4)
)

# This will generate a warning about inconsistent priors
choices_inconsistent <- cbc_choices(design_optimal, priors = different_priors)
```

# Best Practices

## Prior Specification

- **Use realistic priors**: Base on literature, pilot studies, or expert judgment
- **Match design priors**: Use same priors for design optimization and choice simulation  
- **Test multiple scenarios**: Simulate under optimistic and conservative assumptions
- **Include heterogeneity**: Use random parameters when appropriate

## Validation Steps

1. **Check choice counts**: Verify one choice per question
2. **Examine patterns**: Ensure choices align with prior expectations
3. **Test extremes**: Simulate with very strong/weak preferences
4. **Compare methods**: Test different simulation approaches

# Next Steps

After simulating choices:

1. **Conduct power analysis** using `cbc_power()` to determine sample size requirements
2. **Compare designs** by simulating choices for different design methods
3. **Validate assumptions** by checking if simulated patterns match expectations
4. **Refine priors** based on simulation results before data collection

For details on power analysis, see the "Power Analysis" vignette.
