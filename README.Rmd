---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  fig.path = "man/figures/README-",
  fig.retina = 3
)
```

# cbcTools

<!-- badges: start -->
[![CRAN
status](https://www.r-pkg.org/badges/version/cbcTools)](https://CRAN.R-project.org/package=cbcTools)
<!-- badges: end -->

This package contains tools for designing surveys and conducting power analyses for choice based conjoint survey experiments in R.

## Installation

The current version is not yet on CRAN, but you can install it from
Github using the {remotes} library:

```{r, eval=FALSE}
# install.packages("remotes")
remotes::install_github("jhelvy/cbcTools")
```

Load the library with:

```{r}
library("cbcTools")
```

## Make survey designs

The first step in designing an experiment is to define the attributes and levels for your experiment. Many of the functions in {cbcTools} are more convenient to use if you define these as a separate object. For example, let's say you're designing a conjoint experiment about apples. You might have the following attributes and levels:

```{r}
levels <- list(
  price     = seq(1, 4, 0.5), # $ per pound
  type      = c('Fuji', 'Gala', 'Honeycrisp'),
  freshness = c('Excellent', 'Average', 'Poor')
)
```

With these levels defined, you can then obtain all of the `profiles` of each possible combination of the attributes and levels using the `cbc_profiles()` function:

```{r}
profiles <- cbc_profiles(levels)
head(profiles)
```

Depending on the context of your survey, you may wish to eliminate or modify some profiles before designing your conjoint survey (e.g., some profile combinations may be illogical or unrealistic), though doing so could significantly impact your ability to identify effects. As a result, it is recommended that you avoid eliminating profiles if possible.

With these profiles, a randomized conjoint survey can then be generated using the `cbc_design()` function:

```{r}
design <- cbc_design(
  profiles = profiles,
  n_resp   = 300, # Number of respondents
  n_alts   = 3,   # Number of alternatives per question
  n_q      = 6    # Number of questions per respondent
)

dim(design)  # View dimensions
head(design) # Preview first 6 rows
```

For now, the `cbc_design()` function only generates a randomized design. Other packages, such as the [{idefix}](https://github.com/traets/idefix) package, are able to generate other types of designs, such as D-efficient designs. The randomized design simply samples from the set of `profiles`. It also ensures that no two alternatives are the same in any choice question.

The resulting `design` data frame includes the following columns:

- `respID`: Identifies each survey respondent.
- `qID`: Identifies the choice question answered by the respondent.
- `altID`:Identifies the alternative in any one choice observation.
- `obsID`: Identifies each unique choice observation across all respondents.
- `profileID`: Identifies the profile in `profiles`.

### Labeled designs (a.k.a. "alternative-specific" designs)

You can also make a "labeled" design (also known as "alternative-specific" design) where the levels of one attribute is used as a label by setting the `label` argument to that attribute. This by definition sets the number of alternatives in each question to the number of levels in the chosen attribute, so the `n_alts` argument is overridden. Here is an example labeled survey using the `type` attribute as the label:

```{r}
design_labeled <- cbc_design(
  profiles  = profiles,
  n_resp    = 300, # Number of respondents
  n_alts    = 3,   # Number of alternatives per question
  n_q       = 6,   # Number of questions per respondent
  label     = "type" # Set the "type" attribute as the label
)

dim(design_labeled)
head(design_labeled)
```

In the above example, you can see in the first six rows of the survey that the `type` attribute is always fixed to be the same order, ensuring that each level in the `type` attribute will always be shown in each choice question.

### Adding a "no choice" option (a.k.a. "outside good")

You can include a "no choice" (also known as "outside good" option in your survey by setting `no_choice = TRUE`. If included, all categorical attributes will be dummy-coded to appropriately dummy-code the "no choice" alternative.

```{r}
design_nochoice <- cbc_design(
  profiles  = profiles,
  n_resp    = 300, # Number of respondents
  n_alts    = 3, # Number of alternatives per question
  n_q       = 6, # Number of questions per respondent
  no_choice = TRUE
)

dim(design_nochoice)
head(design_nochoice)
```

## Inspect survey design

The {cbcTools} package includes some functions to quickly inspect some basic metrics of a design.

The `cbc_balance()` function prints out a summary of the counts of each level for each attribute across all choice questions as well as the two-way counts across all pairs of attributes:

```{r}
cbc_balance(design)
```

The `cbc_overlap()` function prints out a summary of the amount of "overlap" across attributes within the choice questions. For example, for each attribute, the count under `"1"` is the number of choice questions in which the same level was shown across all alternatives for that attribute (because there was only one level shown). Likewise, the count under `"2"` is the number of choice questions in which only two unique levels of that attribute were shown, and so on:

```{r}
cbc_overlap(design)
```

## Simulate choices

You can simulate choices for a given `design` using the `cbc_choices()` function. By default, random choices are simulated:

```{r}
data <- simulateChoices(
    survey = survey,
    obsID  = "obsID"
)
head(data)
```

You can also pass a list of parameters to define a utility model that will be used to simulate choices. In the example below, the choices are simulated using a utility model with the following parameters:

- 1 continuous `price` parameter
- 4 discrete parameters for `type`
- 2 discrete parameters for `freshness`

```{r, eval=FALSE}
data <- simulateChoices(
    survey = survey,
    obsID  = "obsID",
    pars = list(
        price     = 0.1,
        type      = c(0.1, 0.2, 0.3, 0.4),
        freshness = c(0.1, -0.1))
)
```

You can also simulate data with more complex models, such as mixed logit models where parameters follow a normal or log-normal distribution across the population, or interaction between parameters. In the example below, the choices are simulated using a utility model with the following parameters:

- 1 continuous "price" parameter
- 4 discrete parameters for "type"
- 2 random normal discrete parameters for "freshness"
- 2 interaction parameters between "price" and "freshness"

The `randN()` function is use to make the 2 `freshness` parameters follow a normal distribution with a specified mean (`mu`) and standard deviation (`sigma`).

```{r, eval=FALSE}
data <- simulateChoices(
    survey = survey,
    obsID  = "obsID",
    pars = list(
        price     = 0.1,
        type      = c(0.1, 0.2, 0.3, 0.4),
        freshness = randN(mu = c(0.1, -0.1), sigma = c(1, 2)),
        `price*freshness` = c(1, 2))
)
```

## Conduct a power analysis

The simulated choice data can be used to conduct a power analysis by estimating multiple models with different sample sizes. The `estimateModels()` function achieves this by partitioning the simulated choice data into multiple sizes (defined by the `nbreaks` argument) and then estimating a user-defined choice model on each data subset. In the example below, 10 different sample sizes are used to estimate 10 models.

```{r}
models <- estimateModels(
    nbreaks = 10,
    data    = data,
    pars    = c("price", "type", "freshness"),
    outcome = "choice",
    obsID   = "obsID"
)
```

The resulting `models` object is a list of estimated models, each estimated using the [{logitr}](https://jhelvy.github.io/logitr) package.

While the `models` object is a rather complex object in that it contains multiple models, helper functions can be used to extract information of interest. For example, the estimated coefficients and standard errors from each model can be extracted using the `getModelResults()` function:

```{r}
results <- getModelResults(models)

head(results)
```

Here is a summary of the standard errors for each sample size:

```{r}
library(ggplot2)

ggplot(results) +
  geom_hline(yintercept = 0.05, color = "red", linetype = 2) +
  geom_point(aes(x = sampleSize, y = se, color = coef)) +
  expand_limits(y = 0) +
  theme_bw()
```

## Author, Version, and License Information

- Author: *John Paul Helveston* https://www.jhelvy.com/
- Date First Written: *October 23, 2020*
- License: [MIT](https://github.com/jhelvy/cbcTools/blob/master/LICENSE.md)

## Citation Information

If you use this package for in a publication, I would greatly appreciate it if you cited it - you can get the citation by typing `citation("cbcTools")` into R:

```{r}
citation("cbcTools")
```
